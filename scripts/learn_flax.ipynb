{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1667d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "\n",
    "import numpy as np # We import the standard NumPy library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cec144",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n",
    "# Create the predict function from a set of parameters\n",
    "def make_predict_pytree(params):\n",
    "    def predict(x):\n",
    "        return jnp.dot(params['W'],x)+params['b']\n",
    "    return predict\n",
    "\n",
    "# Create the loss from the data points set\n",
    "def make_mse_pytree(x_batched,y_batched):\n",
    "    def mse(params):\n",
    "        # Define the squared loss for a single pair (x,y)\n",
    "        def squared_error(x,y):\n",
    "            y_pred = make_predict_pytree(params)(x)\n",
    "            return jnp.inner(y-y_pred,y-y_pred)/2.0\n",
    "        # We vectorize the previous to compute the average of the loss on all samples.\n",
    "        return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "    return jax.jit(mse) # And finally we jit the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9407b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': DeviceArray([[-1.9287353e+00,  4.2963773e-01,  7.1613431e-01,\n",
       "                2.1056123e+00,  5.0405198e-01, -2.4983377e+00,\n",
       "               -6.3854122e-01, -2.2620230e+00, -1.3365206e+00,\n",
       "               -2.0426056e-01],\n",
       "              [ 1.1999468e+00, -9.4563615e-01, -1.0878406e+00,\n",
       "               -7.0340687e-01,  3.3224657e-01,  1.7538793e+00,\n",
       "               -7.1916497e-01,  1.0927429e+00, -1.4491038e+00,\n",
       "                5.9715652e-01],\n",
       "              [-1.4826512e+00, -7.6116550e-01,  2.2319783e-01,\n",
       "               -3.0392045e-01,  3.0397046e+00, -3.8419533e-01,\n",
       "               -1.8290077e+00, -2.3353386e+00, -1.1087129e+00,\n",
       "               -7.7454048e-01],\n",
       "              [ 8.2374370e-01, -9.9650651e-01, -7.6030153e-01,\n",
       "                6.3919228e-01, -6.0864404e-02, -1.0859709e+00,\n",
       "                1.2923390e+00, -4.9342966e-01, -1.4719218e-03,\n",
       "                1.2977620e+00],\n",
       "              [-4.5656392e-01, -1.3063020e-01, -3.9179036e-01,\n",
       "                2.1743817e+00, -5.3948894e-02,  4.5653141e-01,\n",
       "               -8.5279357e-01,  1.1709602e+00,  9.6438843e-01,\n",
       "               -2.3813486e-02]], dtype=float32),\n",
       " 'b': DeviceArray([ 1.0923611,  1.312107 , -2.9304829, -0.6492366,  1.1531252],            dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate MSE for our samples\n",
    "mse_pytree = make_mse_pytree(x_samples,y_samples)\n",
    "\n",
    "# Initialize estimated W and b with zeros.\n",
    "params = {'W': jnp.zeros_like(W), 'b': jnp.zeros_like(b)}\n",
    "\n",
    "jax.grad(mse_pytree)(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453c1ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639796\n",
      "Loss step 0:  11.096582\n",
      "Loss step 5:  1.1743387\n",
      "Loss step 10:  0.3287934\n",
      "Loss step 15:  0.1398177\n",
      "Loss step 20:  0.07359567\n",
      "Loss step 25:  0.04415302\n",
      "Loss step 30:  0.029408723\n",
      "Loss step 35:  0.021554684\n",
      "Loss step 40:  0.017227953\n",
      "Loss step 45:  0.014798909\n",
      "Loss step 50:  0.013420274\n",
      "Loss step 55:  0.012632738\n",
      "Loss step 60:  0.012181121\n",
      "Loss step 65:  0.011921484\n",
      "Loss step 70:  0.011772007\n",
      "Loss step 75:  0.01168586\n",
      "Loss step 80:  0.011636183\n",
      "Loss step 85:  0.011607513\n",
      "Loss step 90:  0.011590973\n",
      "Loss step 95:  0.01158142\n",
      "Loss step 100:  0.011575905\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimated W and b with zeros.\n",
    "What = jnp.zeros_like(W)\n",
    "bhat = jnp.zeros_like(b)\n",
    "\n",
    "alpha = 0.3 # Gradient step size\n",
    "print('Loss for \"true\" W,b: ', mse(W,b))\n",
    "for i in range(101):\n",
    "    # We perform one gradient update\n",
    "    What, bhat = What - alpha*jax.grad(mse,0)(What,bhat), bhat - alpha*jax.grad(mse,1)(What,bhat)\n",
    "    if (i%5==0):\n",
    "        print(\"Loss step {}: \".format(i), mse(What,bhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
